{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6364539",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import acquire as aq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3268736",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://codeup.com/data-science/math-in-data-science/'\n",
    "headers = {'User-Agent': 'Codeup Data Science'} # Some websites don't accept the pyhon-requests default user-agent\n",
    "response = get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3063a8bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(response.text[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01febde",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd712d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92492774",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title.string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98882f75",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "By the end of this exercise, you should have a file named acquire.py that contains the specified functions. If you wish, you may break your work into separate files for each website (e.g. acquire_codeup_blog.py and acquire_news_articles.py), but the end function should be present in acquire.py (that is, acquire.py should import get_blog_articles from the acquire_codeup_blog module.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390f26e1",
   "metadata": {},
   "source": [
    "# 1. Codeup Blog Articles\n",
    "\n",
    "Visit Codeup's Blog and record the urls for at least 5 distinct blog posts. For each post, you should scrape at least the post's title and content.\n",
    "\n",
    "Encapsulate your work in a function named __get_blog_articles__ that will return a list of dictionaries, with each dictionary representing one article. The shape of each dictionary should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bd2835",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    'title': 'the title of the article',\n",
    "    'content': 'the full text content of the article'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e00612",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://codeup.com/blog/'\n",
    "headers = {'user-agent': 'Kalpana Data Science Cohort'}\n",
    "response = get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369a6d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.text[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b4db60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "soup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139e65a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0a81df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267a1d22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa1d479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blog_articles(use_cache=True):\n",
    "    \n",
    "    # establish a filename for the local csv\n",
    "    filename = 'codeup_blog_articles.csv'\n",
    "    \n",
    "    if use_cache:\n",
    "        \n",
    "        # check to see if a local copy already exists\n",
    "        if os.path.exists(filename):\n",
    "            print('Reading from local CSV...')\n",
    "            # if so, return the local csv\n",
    "            return pd.read_csv(filename)\n",
    "        \n",
    "    # otherwise, scrape the data from codeup.com\n",
    "    print('Gathering blog articles from codeup.com...')\n",
    "    \n",
    "    articles = []\n",
    "\n",
    "    # go to blog homepage\n",
    "    url = 'https://codeup.com/blog/'\n",
    "    headers = {'user-agent': 'Kalpana Data Science Cohort'}\n",
    "    response = get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # get url for next page of articles\n",
    "    # (returns None if there are no more pages)\n",
    "    next_page = soup.select_one('.pagination.clearfix').div.a\n",
    "\n",
    "    # get the urls for the rest of the articles on this page\n",
    "    urls = []\n",
    "    for article in soup.select('article'):\n",
    "        #for link in article.select('.more-link'):\n",
    "        for link in article.select('.entry-featured-image-url'):\n",
    "            urls.append(link.attrs['href'])\n",
    "\n",
    "    # go to each article page\n",
    "    for url in urls:\n",
    "        response = get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # pull article info and append to list\n",
    "        dct = {}\n",
    "        dct['title'] = soup.select_one('.entry-title').text\n",
    "        dct['content'] = soup.select_one('.entry-content').text.strip()\n",
    "        articles.append(dct)\n",
    "\n",
    "    page_counter = 1\n",
    "    print(f'{page_counter} pages complete     ', end='\\r')\n",
    "\n",
    "    # check whether there is a next page\n",
    "    while next_page != None:\n",
    "        # go to the next page\n",
    "        url = next_page.attrs['href']\n",
    "        response = get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # get url for next page of articles\n",
    "        # (this will return None if there are no more pages)\n",
    "        next_page = soup.select_one('.pagination.clearfix').div.a\n",
    "\n",
    "        # get all the urls for articles on this page\n",
    "        urls = []\n",
    "        for article in soup.select('article'):\n",
    "            for link in article.select('.entry-featured-image-url'):\n",
    "                urls.append(link.attrs['href'])\n",
    "\n",
    "        # go to each article page\n",
    "        for url in urls:\n",
    "            response = get(url, headers=headers)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # pull article info and append to list\n",
    "            dct = {}\n",
    "            dct['title'] = soup.select_one('.entry-title').text\n",
    "            dct['content'] = soup.select_one('.entry-content').text.strip()\n",
    "            articles.append(dct)\n",
    "\n",
    "        page_counter += 1\n",
    "        print(f'{page_counter} pages complete     ', end='\\r')\n",
    "        \n",
    "    print(f'{page_counter} pages scraped. No more pages available.')\n",
    "    \n",
    "    articles = pd.DataFrame(articles)\n",
    "    \n",
    "    # cache local copy\n",
    "    print('Writing to local CSV...')\n",
    "    articles.to_csv(filename, index=False)\n",
    "    print('Writing to local CSV complete.')\n",
    "    \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8bca94f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from local CSV...\n"
     ]
    }
   ],
   "source": [
    "df = aq.get_blog_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f23edd34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is a Career in Tech Recession-Proof?</td>\n",
       "      <td>Given the current economic climate, many econo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Codeup X Superhero Car Show &amp; Comic Con</td>\n",
       "      <td>Codeup had a blast at the San Antonio Superher...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What Jobs Can You Get After a Coding Bootcamp?...</td>\n",
       "      <td>If you’re considering a career in web developm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Codeup’s New Dallas Campus</td>\n",
       "      <td>Codeup’s Dallas campus has a new location! For...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Codeup TV Commercial</td>\n",
       "      <td>Codeup has officially made its TV debut! Our c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0               Is a Career in Tech Recession-Proof?   \n",
       "1            Codeup X Superhero Car Show & Comic Con   \n",
       "2  What Jobs Can You Get After a Coding Bootcamp?...   \n",
       "3                         Codeup’s New Dallas Campus   \n",
       "4                               Codeup TV Commercial   \n",
       "\n",
       "                                             content  \n",
       "0  Given the current economic climate, many econo...  \n",
       "1  Codeup had a blast at the San Antonio Superher...  \n",
       "2  If you’re considering a career in web developm...  \n",
       "3  Codeup’s Dallas campus has a new location! For...  \n",
       "4  Codeup has officially made its TV debut! Our c...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f336ad",
   "metadata": {},
   "source": [
    "# 2. News Articles\n",
    "\n",
    "We will now be scraping text data from inshorts, a website that provides a brief overview of many different topics.\n",
    "\n",
    "Write a function that scrapes the news articles for the following topics:\n",
    "\n",
    "\n",
    "- Business\n",
    "\n",
    "\n",
    "- Sports\n",
    "\n",
    "\n",
    "- Technology\n",
    "\n",
    "\n",
    "- Entertainment\n",
    "\n",
    "\n",
    "The end product of this should be a function named get_news_articles that returns a list of dictionaries, where each dictionary has this shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba72dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    'title': 'The article title',\n",
    "    'content': 'The article content',\n",
    "    'category': 'business' # for example\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771078b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://inshorts.com/en/read'\n",
    "response = get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e961ccea",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = []\n",
    "dct = {}\n",
    "dct['title'] = soup.select('.news-card')[0].select('.news-card-title')[0].span.text\n",
    "dct['content'] = soup.select('.news-card')[0].select('.news-card-content')[0].div.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cc9243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_articles(categories=['business', 'sports', \n",
    "                                  'technology', 'entertainment'], \n",
    "                      use_cache=True):\n",
    "    \n",
    "    # establish a filename for the local csv\n",
    "    filename = 'news_articles.csv'\n",
    "    \n",
    "    if use_cache:\n",
    "        # check to see if a local copy already exists\n",
    "        if os.path.exists(filename):\n",
    "            print('Reading from local CSV...')\n",
    "            # if so, return the local csv\n",
    "            return pd.read_csv(filename)\n",
    "        \n",
    "    # otherwise, scrape the data from codeup.com\n",
    "    print('Reading blog articles from inshorts.com...')\n",
    "\n",
    "    articles = []\n",
    "\n",
    "    for category in categories:\n",
    "\n",
    "        url = f'https://inshorts.com/en/read/{category}'\n",
    "        response = get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        for card in soup.select('.news-card'):\n",
    "            dct = {}\n",
    "            dct['title'] = card.select('.news-card-title')[0].span.text\n",
    "            dct['author'] = card.select('.author')[0].text\n",
    "            dct['content'] = card.select_one('.news-card-content').div.text\n",
    "            dct['category'] = category\n",
    "            articles.append(dct)\n",
    "            \n",
    "    articles = pd.DataFrame(articles)\n",
    "    \n",
    "    # cache local copy\n",
    "    print('Writing to local CSV...')\n",
    "    articles.to_csv(filename, index=False)\n",
    "            \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf70ea21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from local CSV...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adani Transmission becomes India's 8th most va...</td>\n",
       "      <td>Hiral Goyal</td>\n",
       "      <td>Adani Transmission has entered the club of Ind...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Musk cites whistleblower's claims in new notic...</td>\n",
       "      <td>Ridham Gambhir</td>\n",
       "      <td>Tesla CEO Elon Musk's legal team has filed ano...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No plan to rebrand Zomato app to Eternal: CEO ...</td>\n",
       "      <td>Hiral Goyal</td>\n",
       "      <td>Zomato CEO Deepinder Goyal clarified in an exc...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cancelling AC, first-class confirmed train tic...</td>\n",
       "      <td>Ridham Gambhir</td>\n",
       "      <td>The Finance Ministry stated that cancellation ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>China arrests over 230 people tied to its larg...</td>\n",
       "      <td>Hiral Goyal</td>\n",
       "      <td>China has announced that 234 people who are su...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title          author  \\\n",
       "0  Adani Transmission becomes India's 8th most va...     Hiral Goyal   \n",
       "1  Musk cites whistleblower's claims in new notic...  Ridham Gambhir   \n",
       "2  No plan to rebrand Zomato app to Eternal: CEO ...     Hiral Goyal   \n",
       "3  Cancelling AC, first-class confirmed train tic...  Ridham Gambhir   \n",
       "4  China arrests over 230 people tied to its larg...     Hiral Goyal   \n",
       "\n",
       "                                             content  category  \n",
       "0  Adani Transmission has entered the club of Ind...  business  \n",
       "1  Tesla CEO Elon Musk's legal team has filed ano...  business  \n",
       "2  Zomato CEO Deepinder Goyal clarified in an exc...  business  \n",
       "3  The Finance Ministry stated that cancellation ...  business  \n",
       "4  China has announced that 234 people who are su...  business  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = aq.get_news_articles()\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0e557e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f71bca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
